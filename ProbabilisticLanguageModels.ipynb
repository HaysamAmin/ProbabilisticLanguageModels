{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# 🧠 Workshop: Building Blocks for AI Agents\n",
    "\n",
    "## NLP Pipeline + Probabilistic Language Models (90-Minute Team Lab)\n",
    "\n",
    "**Objective:**\n",
    "Work in teams of 3 to build a small NLP pipeline and implement unigram and bigram models, culminating in estimating sentence probabilities. Submit your completed Jupyter Notebook via a GitHub link (with `.git` at the end).\n",
    "## Team member names (Team One): \n",
    "1. **Yu Chen Chou** \n",
    "2. **Zhimin Xiong** \n",
    "3. **Haysam Elamin**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## Part 1 – NLP Pipeline\n",
    "\n",
    "### Step 1: Select and Load a Corpus\n",
    "\n",
    "Select a corpus from `nltk`, or upload your own text documents. Ensure your vocabulary size exceeds 2000 words.\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "We are utilizing information available on the official Conestoga College website as our primary source of data. This includes publicly accessible content such as program details, course descriptions, admission requirements, and student services. All data is being referenced for academic and research purposes to support our analysis and project development.\n",
    "\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\hitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "corpus_docs = [reuters.raw(fileid) for fileid in reuters.fileids()]\n",
    "corpus_text = \" \".join(corpus_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a49c6c",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** This corpus is pre-tokenized and covers multiple topics. It’s a good fit to get us above the 2,000-word vocabulary requirement.\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "**Student Talking Point**: The current corpus is not yet tokenized and is based on information collected from the official Conestoga College website, which serves as our primary data source. It is being prepared to meet the 2,000-word vocabulary requirement for further processing and analysis.\n",
    "++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1158",
   "metadata": {},
   "source": [
    "### Step 2: Collect and Preprocess Documents\n",
    "\n",
    "Convert your corpus into tokens and compute the vocabulary size.\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "We are using the `word_tokenize` This specific function splits a string of text into a list of words and punctuation (called tokens).\n",
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc40f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hitha/nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\share\\\\nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hitha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m      3\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m vocab = \u001b[38;5;28mset\u001b[39m(tokens)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI Courses\\ML -Programing\\Week 12\\New\\ProbabilisticLanguageModels\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hitha/nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\share\\\\nltk_data'\n    - 'e:\\\\AI Courses\\\\ML -Programing\\\\Week 12\\\\New\\\\ProbabilisticLanguageModels\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hitha\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt') ###################################################33\n",
    "tokens = word_tokenize(corpus_text.lower())\n",
    "vocab = set(tokens)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16773306",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Student Talking Point: (token vs vocab )\n",
    "- A **token** is a single unit of text—such as a word or punctuation mark—produced when a sentence or paragraph is broken down through tokenization. \n",
    "- The **vocabulary**, on the other hand, is the set of unique tokens found in the entire text. While tokens may include repeated words, the vocabulary contains each word only once, with duplicates removed.\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227eccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\StudentWork\\Code\\PROG8245\\\n",
      "[nltk_data]     ProbabilisticLanguageModels\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to c:\\StudentWork\\Code\\PROG8\n",
      "[nltk_data]     245\\ProbabilisticLanguageModels\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Paths: ['C:\\\\Users\\\\Eespinosa/nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Eespinosa\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n",
      "Vocabulary size: 52440\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))\n",
    "\n",
    "tokens = word_tokenize(corpus_text.lower())\n",
    "vocab = set(tokens)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b0c8",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** Vocabulary size is important—it determines the richness of our model. Models trained on small vocabularies can't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a46555",
   "metadata": {},
   "source": [
    "### Step 3: Implement Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokens = simple_tokenizer(corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cee35",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** A simple regex tokenizer gives us control—this is useful when we need to understand every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315d4f5",
   "metadata": {},
   "source": [
    "### Step 4: Normalization, Stemming, and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eespinosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "normalized_tokens = normalize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25579af",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** Normalization makes the data more consistent and shrinks the vocabulary. This is essential for estimating reliable probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 – Probabilistic Language Models\n",
    "\n",
    "### 📘 Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n",
    "The probability of a sequence of words $w_1, w_2, ..., w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "To estimate $P(w_i)$, we use the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{j} \\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "where $j$ is the total number of words in the corpus.\n",
    "\n",
    "This is a strong simplification, but it provides a foundational baseline and helps reduce data sparsity in low-resource environments.\n",
    "\n",
    "Here's how to implement it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('japan') = 0.00185\n",
      "P('citibank') = 0.00005\n",
      "P('harvest') = 0.00024\n",
      "P('bank') = 0.00493\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unigram_counts = Counter(normalized_tokens)\n",
    "total_words = len(normalized_tokens)\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words\n",
    "\n",
    "print(f\"P('japan') = {unigram_prob('japan'):.5f}\")\n",
    "print(f\"P('citibank') = {unigram_prob('citibank'):.5f}\")\n",
    "print(f\"P('harvest') = {unigram_prob('harvest'):.5f}\")\n",
    "print(f\"P('bank') = {unigram_prob('bank'):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "##### 📘 Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "In our case, the total number of tokens is quite large:\n",
    "\n",
    "- **Total tokens:** 1,178,604  \n",
    "- **Unique words (vocabulary size):** 67,151\n",
    "\n",
    "Even if a word appears frequently, its probability will still be small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"bank\"` appears quite often, yet its probability is only **0.00493**, or about **0.5%** of the total words.\n",
    "- `\"citibank\"` appears only a few times, resulting in a much smaller probability of **0.00005**.\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is **large and diverse** (like Reuters).\n",
    "- Many words appear **only once or twice**, which is common in natural language (known as Zipf's Law).\n",
    "\n",
    "**Conclusion:**  \n",
    "Low unigram probabilities do **not** indicate an error—they reflect a realistic distribution of word frequencies across a large corpus. This also highlights the need for smoothing when building more complex language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### 📘 Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb0cee",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** Unigram models assume word independence—useful but limited since word order is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382179640797073e-37\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        prob *= unigram_prob(word)\n",
    "    return prob\n",
    "\n",
    "sentence = \"The agreement calls for joint Sino-Chilean management of the venture for 15 years, the paper said\"\n",
    "print(sentence_prob_unigram(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "##### 📘 Why Is the Sentence Probability So Low?\n",
    "\n",
    "The calculated **unigram sentence probability** is:\n",
    "\n",
    "```python\n",
    "2.382179640797073e-37\n",
    "````\n",
    "\n",
    "This number is extremely small—but **that’s expected** for long sentences under a unigram model. Here's why:\n",
    "\n",
    "\n",
    "##### 🔢 Corpus Statistics\n",
    "\n",
    "* **Total number of tokens:** 1,178,604\n",
    "* **Vocabulary size (unique tokens):** 67,151\n",
    "\n",
    "##### 📉 How the Unigram Model Works\n",
    "\n",
    "The unigram model computes sentence probability as the **product of individual word probabilities**:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "Each word typically has a probability between 0.00001 and 0.01. When multiplying **10–20 small numbers together**, the final result becomes **exponentially smaller**, approaching zero for longer sentences.\n",
    "\n",
    "##### 🧪 Impact of Preprocessing (Step 4)\n",
    "\n",
    "The normalization step involves:\n",
    "\n",
    "* Lowercasing\n",
    "* **Stop word removal** (e.g., \"the\", \"of\", \"for\", \"said\")\n",
    "* **Stemming** (e.g., \"management\" → \"manag\")\n",
    "* **Punctuation removal**\n",
    "\n",
    "This reduces the number of words used in the calculation. While this makes the vocabulary smaller and more manageable, it also means:\n",
    "\n",
    "* **Common but removed words** (like \"the\") don’t contribute to the probability.\n",
    "* **Stemmed forms** may not match original unigrams perfectly (e.g., “sino-chilean” becomes `sinochilean` or `sino` and `chilean`, depending on the tokenizer).\n",
    "\n",
    "So even though the sentence appears long, **only 7–12 stemmed and filtered tokens** may remain after preprocessing—yet each one still has a very small individual probability.\n",
    "\n",
    "##### ✅ Key Takeaways\n",
    "\n",
    "* Low sentence probabilities are **normal** in unigram models, especially for longer sentences.\n",
    "* The **multiplicative nature** of probability and the **sparsity of natural language** lead to very small final values.\n",
    "* These limitations are one reason why more advanced models (like bigrams or neural LMs) are needed for realistic NLP applications.\n",
    "\n",
    "You can inspect the intermediate tokens like this:\n",
    "\n",
    "```python\n",
    "print(normalize(simple_tokenizer(sentence)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### 📘 Bigram Model with MLE – Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47092802",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** This simple multiplication illustrates the chain rule, but we’ll soon see how to improve this with context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74b7c",
   "metadata": {},
   "source": [
    "### 📘 Sentence Probability with Bigram Model – Mathematical Explanation\n",
    "\n",
    "Using the bigram model and chain rule:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdots P(w_n | w_{n-1})\n",
    "$$\n",
    "This models **local dependencies** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(normalized_tokens) - 1):\n",
    "    pair = (normalized_tokens[i], normalized_tokens[i + 1])\n",
    "    bigram_counts[pair] += 1\n",
    "\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a63c06",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** Bigram probabilities model word context, capturing more meaning than unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6991255325977075e-21\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    prob = 1.0\n",
    "    for i in range(len(words) - 1):\n",
    "        prob *= bigram_prob(words[i], words[i + 1])\n",
    "    return prob\n",
    "\n",
    "sentence = \"The agreement calls for joint Sino-Chilean management of the venture for 15 years, the paper said\"\n",
    "print(sentence_prob_bigram(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447d671",
   "metadata": {},
   "source": [
    "**👨‍🏫 Professor Talking Point:** Estimating sentence probability using bigrams shows how sequence information improves prediction power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6b238",
   "metadata": {},
   "source": [
    "## Part 3: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac59089",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7a67a",
   "metadata": {},
   "source": [
    "## 🧠 Learning Objectives\n",
    "- Implement the foundations of **Probabilistic Language Models** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(20 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(65 min)* – NLP Pipeline and four Probabilistic Language Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(5 min)* – Teams commit and push the one notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(1 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Probabilistic Language Models Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `ProbabilisticLanguageModels.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the four methods.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `ProbabilisticLanguageModels`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737ca3",
   "metadata": {},
   "source": [
    "## 🧭 Conclusion\n",
    "\n",
    "Today you’ve constructed your own basic language model. Next class, we’ll expand these ideas to explore **Large Language Models (LLMs)**—like ChatGPT—which learn patterns over **massive corpora** using **deep neural networks** instead of just counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
